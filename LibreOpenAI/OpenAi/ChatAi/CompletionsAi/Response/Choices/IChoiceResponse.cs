using LibreOpenAI.OpenAi.ChatAi.CompletionsAi.Response.Choices.Logprobs;
using LibreOpenAI.OpenAi.ChatAi.CompletionsAi.Response.Choices.Message;
using Newtonsoft.Json;

namespace LibreOpenAI.OpenAi.ChatAi.CompletionsAi.Response.Choices
{
    public interface IChoiceResponse
    {
        /// <summary>
        /// The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.
        /// </summary>
        [JsonProperty("finish_reason")]
        string FinishReason { get; set; }
        /// <summary>
        /// The index of the choice in the list of choices.
        /// </summary>
        [JsonProperty("index")]
        int Index { get; set; }
        /// <summary>
        /// Log probability information for the choice.
        /// </summary>
        [JsonProperty("logprobs")]
        LogprobsChoiseResponse? Logprobs { get; set; }
        /// <summary>
        /// A chat completion message generated by the model.
        /// </summary>
        [JsonProperty("message")]
        MessageChoiseResponse Message { get; set; }
    }
}